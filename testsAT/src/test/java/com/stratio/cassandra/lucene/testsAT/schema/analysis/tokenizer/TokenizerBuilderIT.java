package com.stratio.cassandra.lucene.testsAT.schema.analysis.tokenizer;

import com.stratio.cassandra.lucene.builder.index.schema.analysis.tokenizer.*;
import com.stratio.cassandra.lucene.testsAT.BaseIT;
import com.stratio.cassandra.lucene.testsAT.util.CassandraUtils;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

import static com.stratio.cassandra.lucene.builder.Builder.*;

/**
 * Test partitioning on partition key column.
 *
 * @author Andres de la Pena {@literal <adelapena@stratio.com>}
 */
@RunWith(JUnit4.class)
public class TokenizerBuilderIT extends BaseIT{

    private static CassandraUtils utils;

    @BeforeClass
    public static void before() {}

    @AfterClass
    public static void after() {
        CassandraUtils.dropKeyspaceIfNotNull(utils);
    }

    @Test
    public void testClassicTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new ClassicTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(1)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(1)
                .filter(match("rc", "-Foxes")).check(1)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(1)
                .filter(match("rc", "or")).check(1)
                .filter(match("rc", "and/or")).check(1)
                .filter(match("rc", "and*or")).check(1)
                .filter(match("rc", "and-or")).check(1)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(phrase("rc", "  dog bone and/or")).check(1)
                .filter(phrase("rc", "  dog bone/ and*or")).check(1)
                .filter(phrase("rc", "  dog Bone. and/or")).check(0)
                .filter(phrase("rc", "  dog bone. and or")).check(1)
                .filter(phrase("rc", "  dog bone and or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                //TODO: check this behaviour
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and*or*")).check(0)
                .filter(wildcard("rc", "and/or*")).check(0)
                .filter(wildcard("rc", "*and/or")).check(0)
                .filter(wildcard("rc", "and*or")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*.and.*")).check(0);
    }

    @Test
    public void testNGramTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new NGramTokenizer(1, 2)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "abcde")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a")).check(1)
                .filter(match("rc", "ab")).check(1)
                .filter(match("rc", "abc")).check(1)
                .filter(match("rc", "abcde")).check(1)
                .filter(match("rc", "cd")).check(1)
                .filter(match("rc", "de")).check(1)
                .filter(match("rc", "be")).check(0)
                .filter(phrase("rc", "a")).check(1)
                .filter(phrase("rc", "ab")).check(1)
                .filter(phrase("rc", "abc")).check(1)
                .filter(phrase("rc", "abcde")).check(1)
                .filter(phrase("rc", "cd")).check(1)
                .filter(phrase("rc", "de")).check(1)
                .filter(phrase("rc", "be")).check(0)
                .filter(fuzzy("rc", "a")).check(1)
                .filter(fuzzy("rc", "ab")).check(1)
                .filter(fuzzy("rc", "abc")).check(1)
                .filter(fuzzy("rc", "cd")).check(1)
                .filter(fuzzy("rc", "de")).check(1)
                .filter(contains("rc", "a")).check(1)
                .filter(contains("rc", "abc")).check(1)
                .filter(contains("rc", "be")).check(0)
                .filter(prefix("rc", "ab")).check(1)
                .filter(prefix("rc", "bc")).check(1)
                .filter(regexp("rc", "[aeiou]{1}bc*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*cde*")).check(0);
    }

    @Test
    public void testKeywordTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new KeywordTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the")).check(0)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the")).check(0)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/o")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.")).check(0)
                //TODO: check this behaviour
                .filter(contains("rc", "The 2 QUICK Brown-Foxes")).check(0)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "and/or")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "The")).check(1)
                .filter(prefix("rc", "ju")).check(0)
                .filter(wildcard("rc", "*2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(wildcard("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.*")).check(1)
                .filter(wildcard("rc", "*Brown-Foxes jumped the lazy dog bone.*")).check(1)
                .filter(wildcard("rc", "*QUICK * jumped*")).check(1);

    }

    @Test
    public void testLetterTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new LetterTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/o")).check(0)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.")).check(0)
                //TODO: check this behaviour
                .filter(contains("rc", "The 2 QUICK Brown-Foxes")).check(1)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "and/or")).check(1)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "The")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(wildcard("rc", "*")).check(1)
                .filter(wildcard("rc", "*QUICK*")).check(1);
    }

    @Test
    public void testLowerCaseTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new LowerCaseTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/o")).check(0)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.")).check(0)
                //TODO: check this behaviour
                .filter(contains("rc", "The 2 QUICK Brown-Foxes")).check(1)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "and/or")).check(1)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "the")).check(1)
                .filter(prefix("rc", "The")).check(0)
                .filter(prefix("rc", "ju")).check(1)
                .filter(wildcard("rc", "*")).check(1)
                .filter(wildcard("rc", "*quick*")).check(1);
    }

    @Test
    public void testEdgeNGramTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new EdgeNGramTokenizer(1, 2)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "abcde")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a")).check(1)
                .filter(match("rc", "ab")).check(1)
                .filter(match("rc", "abc")).check(1)
                .filter(match("rc", "abcde")).check(1)
                .filter(match("rc", "cd")).check(0)
                .filter(match("rc", "de")).check(0)
                .filter(match("rc", "be")).check(0)
                .filter(phrase("rc", "a")).check(1)
                .filter(phrase("rc", "ab")).check(1)
                .filter(phrase("rc", "abc")).check(1)
                .filter(phrase("rc", "abcde")).check(1)
                .filter(phrase("rc", "cd")).check(0)
                .filter(phrase("rc", "de")).check(0)
                .filter(phrase("rc", "be")).check(0)
                .filter(fuzzy("rc", "a")).check(1)
                .filter(fuzzy("rc", "ab")).check(1)
                .filter(fuzzy("rc", "abc")).check(1)
                .filter(fuzzy("rc", "cd")).check(0)
                .filter(fuzzy("rc", "de")).check(0)
                .filter(contains("rc", "a")).check(1)
                .filter(contains("rc", "abc")).check(1)
                .filter(contains("rc", "be")).check(0)
                .filter(prefix("rc", "ab")).check(1)
                .filter(prefix("rc", "bc")).check(0)
                .filter(regexp("rc", "[aeiou]{1}bc*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*cde*")).check(0);
    }

    @Test
    public void testPathHierarchyTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new PathHierarchyTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "/a/b/c/d/e/f/g")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "/a/c")).check(1)
                .filter(match("rc", "/ac")).check(0)
                .filter(match("rc", "/a/e")).check(1)
                .filter(match("rc", "/a/e/g")).check(1)
                .filter(match("rc", "/a")).check(1)
                .filter(match("rc", "/a/b")).check(1)
                .filter(match("rc", "/a/b/c")).check(1)
                .filter(fuzzy("rc", "/a")).check(1)
                .filter(fuzzy("rc", "/a/c")).check(1)
                .filter(fuzzy("rc", "a/e/g")).check(0)
                .filter(match("rc", "abc")).check(0)
                .filter(contains("rc", "/a")).check(1)
                .filter(contains("rc", "/a/b/c")).check(1)
                .filter(contains("rc", "b/c")).check(0)
                .filter(prefix("rc", "/a/b")).check(1)
                .filter(prefix("rc", "b/c")).check(0)
                .filter(regexp("rc", "/[aeiou]{1}/b/c*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "/a*")).check(1)
                .filter(wildcard("rc", "*/c*")).check(1)
                .filter(wildcard("rc", "*c/*")).check(1);
    }

    @Test
    public void testPatternTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new PatternTokenizer("/", 0, -1)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "/a/b/c/d/e/f/g")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a")).check(1)
                .filter(match("rc", "/ac")).check(0)
                .filter(match("rc", "/a/e")).check(0)
                .filter(match("rc", "/a/e/g")).check(0)
                .filter(match("rc", "a/b")).check(1)
                .filter(match("rc", "/a/b/c")).check(1)
                .filter(fuzzy("rc", "a")).check(1)
                .filter(fuzzy("rc", "/a")).check(0)
                .filter(fuzzy("rc", "b")).check(1)
                .filter(fuzzy("rc", "a/e/g")).check(0)
                .filter(match("rc", "abc")).check(0)
                .filter(contains("rc", "/a")).check(1)
                .filter(contains("rc", "/a/b/c")).check(1)
                .filter(contains("rc", "b/c")).check(1)
                .filter(prefix("rc", "a")).check(1)
                .filter(prefix("rc", "b/c")).check(0)
                .filter(regexp("rc", "[aeiou]{1}")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*/c*")).check(0)
                .filter(wildcard("rc", "*c*")).check(1);
    }

    @Test
    public void testReversePathHierarchyTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new ReversePathHierarchyTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "g/f/e/d/c/b/a/")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a/")).check(1)
                .filter(match("rc", "/ac")).check(0)
                .filter(match("rc", "/a/e")).check(0)
                .filter(match("rc", "/a/e/g")).check(0)
                .filter(match("rc", "b/a/")).check(1)
                .filter(match("rc", "c/b/a/")).check(1)
                .filter(match("rc", "abc")).check(0)
                .filter(fuzzy("rc", "a/")).check(1)
                .filter(fuzzy("rc", "/a")).check(1)
                .filter(fuzzy("rc", "b/")).check(1)
                .filter(fuzzy("rc", "a/e/g")).check(0)
                .filter(contains("rc", "a/")).check(1)
                .filter(contains("rc", "c/b/a/")).check(1)
                .filter(contains("rc", "b/c")).check(0)
                .filter(prefix("rc", "a")).check(1)
                .filter(prefix("rc", "b/c")).check(0)
                .filter(regexp("rc", "[aeiou]{1}/")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*c/*")).check(1)
                .filter(wildcard("rc", "*c*")).check(1);
    }

    @Test
    public void testStandardTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new StandardTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(1)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(1)
                .filter(match("rc", "-Foxes")).check(1)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(1)
                .filter(match("rc", "or")).check(1)
                .filter(match("rc", "and/or")).check(1)
                .filter(match("rc", "and*or")).check(1)
                .filter(match("rc", "and-or")).check(1)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(phrase("rc", "  dog bone and/or")).check(1)
                .filter(phrase("rc", "  dog bone/ and*or")).check(1)
                .filter(phrase("rc", "  dog Bone. and/or")).check(0)
                .filter(phrase("rc", "  dog bone. and or")).check(1)
                .filter(phrase("rc", "  dog bone and or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                //TODO: check this behaviour
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and*or*")).check(0)
                .filter(wildcard("rc", "and/or*")).check(0)
                .filter(wildcard("rc", "*and/or")).check(0)
                .filter(wildcard("rc", "and*or")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*.and.*")).check(0);
    }

    @Test
    public void testThaiTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new ThaiTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "การที่ได้ต้องแสดงว่างานดี") // { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี" }
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "การ")).check(1)
                .filter(match("rc", "การที่")).check(1)
                .filter(phrase("rc", "ได้ ต้อง")).check(1)
                .filter(phrase("rc", "แสดง งาน")).check(0)
                .filter(fuzzy("rc", "การ")).check(1)
                .filter(fuzzy("rc", "การว่า")).check(0)
                .filter(contains("rc", "สดง")).check(0)
                .filter(prefix("rc", "กา")).check(1)
                .filter(prefix("rc", "งาน")).check(1)
                .filter(wildcard("rc", "*การ*")).check(1)
                .filter(wildcard("rc", "การ*")).check(1)
                .filter(wildcard("rc", "*การแสดง*")).check(0);
    }

    @Test
    public void testUAX29URLEmailTokenizerTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new UAX29URLEmailTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "Email me at john.smith@global-international.com")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "john.smith@global-international.com")).check(1)
                .filter(match("rc", "Email")).check(1)
                .filter(phrase("rc", "me at john.smith@global-international.com")).check(1)
                .filter(phrase("rc", "at me")).check(0)
                .filter(fuzzy("rc", "john.smith@global-international.com")).check(1)
                .filter(fuzzy("rc", "john.smith@global-international.")).check(0)
                .filter(contains("rc", "john.smith@global-")).check(0)
                .filter(prefix("rc", "john.smith")).check(1)
                .filter(wildcard("rc", "*global-international.com*")).check(1)
                .filter(wildcard("rc", "*jhon*global-international.com")).check(0);
    }

    @Test
    public void testUnicodeWhiteSpaceTokenizerTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new UnicodeWhitespaceTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(0)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(0)
                .filter(match("rc", "-Foxes")).check(0)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(0)
                .filter(match("rc", "or")).check(0)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                //TODO: check this behaviour
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and/or*")).check(1)
                .filter(wildcard("rc", "*and/or")).check(1)
                .filter(wildcard("rc", "and*or")).check(1);
    }

    @Test
    public void testWhiteSpaceTokenizerTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new WhitespaceTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(0)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(0)
                .filter(match("rc", "-Foxes")).check(0)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(0)
                .filter(match("rc", "or")).check(0)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                //TODO: check this behaviour
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and/or*")).check(1)
                .filter(wildcard("rc", "*and/or")).check(1)
                .filter(wildcard("rc", "and*or")).check(1);
    }

    @Test
    public void testWikipediaTokenizerTokenizer() {
        String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] "
                + "Category This is (parens) This is a [[link]]  This is an external URL [http://lucene.apache.org] "
                + "Here is ''italics'' and ''more italics'', '''bold''' and '''''five quotes''''' "
                + " This is a [[link|display info]]  This is a period.  Here is $3.25 and here is 3.50.  Here's Johnny.  "
                + "==heading== ===sub head=== followed by some text  [[Category:blah| ]] "
                + "''[[Category:ital_cat]]''  here is some that is ''italics [[Category:foo]] but is never closed."
                + "'''same [[Category:foo]] goes for this '''''and2 [[Category:foo]] and this"
                + " [http://foo.boo.com/test/test/ Test Test] [http://foo.boo.com/test/test/test.html Test Test]"
                + " [http://foo.boo.com/test/test/test.html?g=b&c=d Test Test] <ref>Citation</ref> <sup>martian</sup> <span class=\"glue\">code</span>";

        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new WikipediaTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, test)
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "[http://foo.boo.com/test/test/ Test Test] [http://foo.boo.com/test/test/test.html Test Test]")).check(1)
                .filter(match("rc", "http://foo.boo.com/test/test/test.html")).check(0)
                .filter(match("rc", "''italics''")).check(1)
                .filter(match("rc", "italics")).check(1)
                .filter(phrase("rc", "===sub head=== followed by some text")).check(1)
                .filter(phrase("rc", "sub head followed by some text")).check(1)
                .filter(fuzzy("rc", "sub")).check(1)
                .filter(fuzzy("rc", "sub head followed by some text")).check(0)
                .filter(contains("rc", "===sub head=== followed by some text")).check(1)
                .filter(contains("rc", "sub head followed by some text")).check(1);
    }
}

