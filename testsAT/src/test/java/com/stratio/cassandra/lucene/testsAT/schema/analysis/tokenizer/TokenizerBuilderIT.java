/*
 * Copyright (C) 2014 Stratio (http://stratio.com)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.stratio.cassandra.lucene.testsAT.schema.analysis.tokenizer;

import com.stratio.cassandra.lucene.builder.index.schema.analysis.tokenizer.*;
import com.stratio.cassandra.lucene.testsAT.BaseIT;
import com.stratio.cassandra.lucene.testsAT.util.CassandraUtils;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

import static com.stratio.cassandra.lucene.builder.Builder.*;

/**
 * Test Tokenizers.
 *
 * @author Juan Pedro Gilaberte {@literal <jpgilaberte@stratio.com>}
 */
@RunWith(JUnit4.class)
public class TokenizerBuilderIT extends BaseIT{

    private static CassandraUtils utils;

    @BeforeClass
    public static void before() {}

    @AfterClass
    public static void after() {
        CassandraUtils.dropKeyspaceIfNotNull(utils);
    }

    @Test
    public void testClassicTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new ClassicTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(1)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(1)
                .filter(match("rc", "-Foxes")).check(1)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(1)
                .filter(match("rc", "or")).check(1)
                .filter(match("rc", "and/or")).check(1)
                .filter(match("rc", "and*or")).check(1)
                .filter(match("rc", "and-or")).check(1)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(phrase("rc", "  dog bone and/or")).check(1)
                .filter(phrase("rc", "  dog bone/ and*or")).check(1)
                .filter(phrase("rc", "  dog Bone. and/or")).check(0)
                .filter(phrase("rc", "  dog bone. and or")).check(1)
                .filter(phrase("rc", "  dog bone and or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                //TODO: check this behaviour
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and*or*")).check(0)
                .filter(wildcard("rc", "and/or*")).check(0)
                .filter(wildcard("rc", "*and/or")).check(0)
                .filter(wildcard("rc", "and*or")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*.and.*")).check(0);
    }

    @Test
    public void testNGramTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new NGramTokenizer(1, 2)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "abcde")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a")).check(1)
                .filter(match("rc", "ab")).check(1)
                .filter(match("rc", "abc")).check(1)
                .filter(match("rc", "abcde")).check(1)
                .filter(match("rc", "cd")).check(1)
                .filter(match("rc", "de")).check(1)
                .filter(match("rc", "be")).check(0)
                .filter(phrase("rc", "a")).check(1)
                .filter(phrase("rc", "ab")).check(1)
                .filter(phrase("rc", "abc")).check(1)
                .filter(phrase("rc", "abcde")).check(1)
                .filter(phrase("rc", "cd")).check(1)
                .filter(phrase("rc", "de")).check(1)
                .filter(phrase("rc", "be")).check(0)
                .filter(fuzzy("rc", "a")).check(1)
                .filter(fuzzy("rc", "ab")).check(1)
                .filter(fuzzy("rc", "abc")).check(1)
                .filter(fuzzy("rc", "cd")).check(1)
                .filter(fuzzy("rc", "de")).check(1)
                .filter(contains("rc", "a")).check(1)
                .filter(contains("rc", "abc")).check(1)
                .filter(contains("rc", "be")).check(0)
                .filter(prefix("rc", "ab")).check(1)
                .filter(prefix("rc", "bc")).check(1)
                .filter(regexp("rc", "[aeiou]{1}bc*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*cde*")).check(0);
    }

    @Test
    public void testKeywordTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new KeywordTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the")).check(0)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the")).check(0)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/o")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.")).check(0)
                //TODO: check this behaviour
                .filter(contains("rc", "The 2 QUICK Brown-Foxes")).check(0)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "and/or")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "The")).check(1)
                .filter(prefix("rc", "ju")).check(0)
                .filter(wildcard("rc", "*2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(wildcard("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.*")).check(1)
                .filter(wildcard("rc", "*Brown-Foxes jumped the lazy dog bone.*")).check(1)
                .filter(wildcard("rc", "*QUICK * jumped*")).check(1);

    }

    @Test
    public void testLetterTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new LetterTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/o")).check(0)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.")).check(0)
                .filter(contains("rc", "The 2 QUICK Brown-Foxes")).check(1)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "and/or")).check(1)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "The")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(wildcard("rc", "*")).check(1)
                .filter(wildcard("rc", "*QUICK*")).check(1);
    }

    @Test
    public void testLowerCaseTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new LowerCaseTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(match("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")).check(1)
                .filter(phrase("rc", "The 2 QUICK Brown-Foxes jumped the")).check(1)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/o")).check(0)
                .filter(fuzzy("rc", "The 2 QUICK Brown-Foxes jumped the lazy dog bone.")).check(0)
                .filter(contains("rc", "The 2 QUICK Brown-Foxes")).check(1)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "and/or")).check(1)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "the")).check(1)
                .filter(prefix("rc", "The")).check(0)
                .filter(prefix("rc", "ju")).check(1)
                .filter(wildcard("rc", "*")).check(1)
                .filter(wildcard("rc", "*quick*")).check(1);
    }

    @Test
    public void testEdgeNGramTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new EdgeNGramTokenizer(1, 2)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "abcde")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a")).check(1)
                .filter(match("rc", "ab")).check(1)
                .filter(match("rc", "abc")).check(1)
                .filter(match("rc", "abcde")).check(1)
                .filter(match("rc", "cd")).check(0)
                .filter(match("rc", "de")).check(0)
                .filter(match("rc", "be")).check(0)
                .filter(phrase("rc", "a")).check(1)
                .filter(phrase("rc", "ab")).check(1)
                .filter(phrase("rc", "abc")).check(1)
                .filter(phrase("rc", "abcde")).check(1)
                .filter(phrase("rc", "cd")).check(0)
                .filter(phrase("rc", "de")).check(0)
                .filter(phrase("rc", "be")).check(0)
                .filter(fuzzy("rc", "a")).check(1)
                .filter(fuzzy("rc", "ab")).check(1)
                .filter(fuzzy("rc", "abc")).check(1)
                .filter(fuzzy("rc", "cd")).check(0)
                .filter(fuzzy("rc", "de")).check(0)
                .filter(contains("rc", "a")).check(1)
                .filter(contains("rc", "abc")).check(1)
                .filter(contains("rc", "be")).check(0)
                .filter(prefix("rc", "ab")).check(1)
                .filter(prefix("rc", "bc")).check(0)
                .filter(regexp("rc", "[aeiou]{1}bc*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*cde*")).check(0);
    }

    @Test
    public void testPathHierarchyTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new PathHierarchyTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "/a/b/c/d/e/f/g")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "/a/c")).check(1)
                .filter(match("rc", "/ac")).check(0)
                .filter(match("rc", "/a/e")).check(1)
                .filter(match("rc", "/a/e/g")).check(1)
                .filter(match("rc", "/a")).check(1)
                .filter(match("rc", "/a/b")).check(1)
                .filter(match("rc", "/a/b/c")).check(1)
                .filter(fuzzy("rc", "/a")).check(1)
                .filter(fuzzy("rc", "/a/c")).check(1)
                .filter(fuzzy("rc", "a/e/g")).check(0)
                .filter(match("rc", "abc")).check(0)
                .filter(contains("rc", "/a")).check(1)
                .filter(contains("rc", "/a/b/c")).check(1)
                .filter(contains("rc", "b/c")).check(0)
                .filter(prefix("rc", "/a/b")).check(1)
                .filter(prefix("rc", "b/c")).check(0)
                .filter(regexp("rc", "/[aeiou]{1}/b/c*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "/a*")).check(1)
                .filter(wildcard("rc", "*/c*")).check(1)
                .filter(wildcard("rc", "*c/*")).check(1);
    }

    @Test
    public void testPatternTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new PatternTokenizer("/", -1)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "/a/b/c/d/e/f/g")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a")).check(1)
                .filter(match("rc", "/ac")).check(0)
                .filter(match("rc", "/a/e")).check(0)
                .filter(match("rc", "/a/e/g")).check(0)
                .filter(match("rc", "a/b")).check(1)
                .filter(match("rc", "/a/b/c")).check(1)
                .filter(fuzzy("rc", "a")).check(1)
                .filter(fuzzy("rc", "/a")).check(0)
                .filter(fuzzy("rc", "b")).check(1)
                .filter(fuzzy("rc", "a/e/g")).check(0)
                .filter(match("rc", "abc")).check(0)
                .filter(contains("rc", "/a")).check(1)
                .filter(contains("rc", "/a/b/c")).check(1)
                .filter(contains("rc", "b/c")).check(1)
                .filter(prefix("rc", "a")).check(1)
                .filter(prefix("rc", "b/c")).check(0)
                .filter(regexp("rc", "[aeiou]{1}")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*/c*")).check(0)
                .filter(wildcard("rc", "*c*")).check(1);
    }

    @Test
    public void testReversePathHierarchyTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new PathHierarchyTokenizer(true, '/', '/', 0)))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "g/f/e/d/c/b/a/")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "a/")).check(1)
                .filter(match("rc", "/ac")).check(0)
                .filter(match("rc", "/a/e")).check(0)
                .filter(match("rc", "/a/e/g")).check(0)
                .filter(match("rc", "b/a/")).check(1)
                .filter(match("rc", "c/b/a/")).check(1)
                .filter(match("rc", "abc")).check(0)
                .filter(fuzzy("rc", "a/")).check(1)
                .filter(fuzzy("rc", "/a")).check(1)
                .filter(fuzzy("rc", "b/")).check(1)
                .filter(fuzzy("rc", "a/e/g")).check(0)
                .filter(contains("rc", "a/")).check(1)
                .filter(contains("rc", "c/b/a/")).check(1)
                .filter(contains("rc", "b/c")).check(0)
                .filter(prefix("rc", "a")).check(1)
                .filter(prefix("rc", "b/c")).check(0)
                .filter(regexp("rc", "[aeiou]{1}/")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "a*")).check(1)
                .filter(wildcard("rc", "*c/*")).check(1)
                .filter(wildcard("rc", "*c*")).check(1);
    }

    @Test
    public void testStandardTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new StandardTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(1)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(1)
                .filter(match("rc", "-Foxes")).check(1)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(1)
                .filter(match("rc", "or")).check(1)
                .filter(match("rc", "and/or")).check(1)
                .filter(match("rc", "and*or")).check(1)
                .filter(match("rc", "and-or")).check(1)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(phrase("rc", "  dog bone and/or")).check(1)
                .filter(phrase("rc", "  dog bone/ and*or")).check(1)
                .filter(phrase("rc", "  dog Bone. and/or")).check(0)
                .filter(phrase("rc", "  dog bone. and or")).check(1)
                .filter(phrase("rc", "  dog bone and or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                //TODO: check this behaviour
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and*or*")).check(0)
                .filter(wildcard("rc", "and/or*")).check(0)
                .filter(wildcard("rc", "*and/or")).check(0)
                .filter(wildcard("rc", "and*or")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*and/or*")).check(0)
                .filter(wildcard("rc", "*.and.*")).check(0);
    }

    @Test
    public void testThaiTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new ThaiTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "การที่ได้ต้องแสดงว่างานดี") // { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี" }
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "การ")).check(1)
                .filter(match("rc", "การที่")).check(1)
                .filter(phrase("rc", "ได้ ต้อง")).check(1)
                .filter(phrase("rc", "แสดง งาน")).check(0)
                .filter(fuzzy("rc", "การ")).check(1)
                .filter(fuzzy("rc", "การว่า")).check(0)
                .filter(contains("rc", "สดง")).check(0)
                .filter(prefix("rc", "กา")).check(1)
                .filter(prefix("rc", "งาน")).check(1)
                .filter(wildcard("rc", "*การ*")).check(1)
                .filter(wildcard("rc", "การ*")).check(1)
                .filter(wildcard("rc", "*การแสดง*")).check(0);
    }

    @Test
    public void testUAX29URLEmailTokenizerTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new UAX29URLEmailTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "Email me at john.smith@global-international.com")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "john.smith@global-international.com")).check(1)
                .filter(match("rc", "Email")).check(1)
                .filter(phrase("rc", "me at john.smith@global-international.com")).check(1)
                .filter(phrase("rc", "at me")).check(0)
                .filter(fuzzy("rc", "john.smith@global-international.com")).check(1)
                .filter(fuzzy("rc", "john.smith@global-international.")).check(0)
                .filter(contains("rc", "john.smith@global-")).check(0)
                .filter(prefix("rc", "john.smith")).check(1)
                .filter(wildcard("rc", "*global-international.com*")).check(1)
                .filter(wildcard("rc", "*jhon*global-international.com")).check(0);
    }

    @Test
    public void testUnicodeWhiteSpaceTokenizerTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new WhitespaceTokenizer("unicode")))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(0)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(0)
                .filter(match("rc", "-Foxes")).check(0)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(0)
                .filter(match("rc", "or")).check(0)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and/or*")).check(1)
                .filter(wildcard("rc", "*and/or")).check(1)
                .filter(wildcard("rc", "and*or")).check(1);
    }

    @Test
    public void testWhiteSpaceTokenizerTokenizer() {
        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new WhitespaceTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, "The 2 QUICK Brown-Foxes jumped the lazy dog bone. and/or")
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "The")).check(1)
                .filter(match("rc", "the")).check(1)
                .filter(match("rc", "2")).check(1)
                .filter(match("rc", "QUICK")).check(1)
                .filter(match("rc", "quick")).check(0)
                .filter(match("rc", "Quick")).check(0)
                .filter(match("rc", "Brown")).check(0)
                .filter(match("rc", "brown")).check(0)
                .filter(match("rc", "brOwn")).check(0)
                .filter(match("rc", "Foxes")).check(0)
                .filter(match("rc", "-Foxes")).check(0)
                .filter(match("rc", "Brown-Foxes")).check(1)
                .filter(match("rc", "BrownFoxes")).check(0)
                .filter(match("rc", "brown-Foxes")).check(0)
                .filter(match("rc", "brown-foxes")).check(0)
                .filter(match("rc", "jumped")).check(1)
                .filter(match("rc", "jump")).check(0)
                .filter(match("rc", "dog")).check(1)
                .filter(match("rc", "dogs")).check(0)
                .filter(match("rc", "and")).check(0)
                .filter(match("rc", "or")).check(0)
                .filter(phrase("rc", "jumped the")).check(1)
                .filter(phrase("rc", "jump the")).check(0)
                .filter(phrase("rc", " dog bone. and/or")).check(1)
                .filter(fuzzy("rc", "jumped")).check(1)
                .filter(fuzzy("rc", "jump")).check(1)
                .filter(fuzzy("rc", "jumper")).check(1)
                .filter(fuzzy("rc", "ajumper")).check(1)
                .filter(fuzzy("rc", "gjumperd")).check(1)
                .filter(fuzzy("rc", "dogjumperdog")).check(0)
                .filter(contains("rc", "jumped")).check(1)
                .filter(contains("rc", "jump")).check(0)
                .filter(contains("rc", "jumper")).check(0)
                .filter(contains("rc", "ajumped")).check(0)
                .filter(prefix("rc", "jump")).check(1)
                .filter(prefix("rc", "ju")).check(1)
                .filter(regexp("rc", "[j][aeiou]{1}.*")).check(1)
                .filter(regexp("rc", "[j][aeiou]{2}.*")).check(0)
                .filter(wildcard("rc", "*jumpe*")).check(1)
                .filter(wildcard("rc", "*ju*pe*")).check(1)
                .filter(wildcard("rc", "*jum*pe*")).check(1)
                .filter(wildcard("rc", "jumpe*")).check(1)
                .filter(wildcard("rc", "**e*")).check(1)
                .filter(wildcard("rc", "and/or*")).check(1)
                .filter(wildcard("rc", "*and/or")).check(1)
                .filter(wildcard("rc", "and*or")).check(1);
    }

    @Test
    public void testWikipediaTokenizerTokenizer() {
        String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] "
                + "Category This is (parens) This is a [[link]]  This is an external URL [http://lucene.apache.org] "
                + "Here is ''italics'' and ''more italics'', '''bold''' and '''''five quotes''''' "
                + " This is a [[link|display info]]  This is a period.  Here is $3.25 and here is 3.50.  Here's Johnny.  "
                + "==heading== ===sub head=== followed by some text  [[Category:blah| ]] "
                + "''[[Category:ital_cat]]''  here is some that is ''italics [[Category:foo]] but is never closed."
                + "'''same [[Category:foo]] goes for this '''''and2 [[Category:foo]] and this"
                + " [http://foo.boo.com/test/test/ Test Test] [http://foo.boo.com/test/test/test.html Test Test]"
                + " [http://foo.boo.com/test/test/test.html?g=b&c=d Test Test] <ref>Citation</ref> <sup>martian</sup> <span class=\"glue\">code</span>";

        utils = CassandraUtils.builder("tokenizer")
                .withPartitionKey("pk")
                .withColumn("pk", "int")
                .withColumn("rc", "text", textMapper().analyzer("en"))
                .withAnalyzer("en", customAnalyzer(new WikipediaTokenizer()))
                .build()
                .createKeyspace()
                .createTable()
                .insert("pk,rc", 1, test)
                .createIndex().refresh()
                .filter(all()).check(1)
                .filter(none()).check(0)
                .filter(match("rc", "[http://foo.boo.com/test/test/ Test Test] [http://foo.boo.com/test/test/test.html Test Test]")).check(1)
                .filter(match("rc", "http://foo.boo.com/test/test/test.html")).check(0)
                .filter(match("rc", "''italics''")).check(1)
                .filter(match("rc", "italics")).check(1)
                .filter(phrase("rc", "===sub head=== followed by some text")).check(1)
                .filter(phrase("rc", "sub head followed by some text")).check(1)
                .filter(fuzzy("rc", "sub")).check(1)
                .filter(fuzzy("rc", "sub head followed by some text")).check(0)
                .filter(contains("rc", "===sub head=== followed by some text")).check(1)
                .filter(contains("rc", "sub head followed by some text")).check(1);
    }
}
